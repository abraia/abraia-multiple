{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "cellView": "form",
        "id": "-cp253OYk0zk"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!python -m pip install onnx onnxruntime\n",
        "!python -m pip install abraia\n",
        "\n",
        "import os\n",
        "if not os.getenv('ABRAIA_ID') and not os.getenv('ABRAIA_KEY'):\n",
        "    #@markdown <a href=\"https://abraia.me/console/gallery\" target=\"_blank\">Upload and manage your images</a>\n",
        "    abraia_id = ''  #@param {type: \"string\"}\n",
        "    abraia_key = ''  #@param {type: \"string\"}\n",
        "    %env ABRAIA_ID=$abraia_id\n",
        "    %env ABRAIA_KEY=$abraia_key\n",
        "\n",
        "from abraia import Abraia\n",
        "\n",
        "multiple = Abraia()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "dCkPRZpkJ3Rv"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# import torchvision\n",
        "\n",
        "# dummy_input = torch.randn(1, 3, 224, 224)\n",
        "# model = torchvision.models.mobilenet_v2(pretrained=True)\n",
        "# model.eval()\n",
        "\n",
        "# torch.onnx.export(model, dummy_input, \"model.onnx\", verbose=True, input_names=['input'], output_names=['output'])\n",
        "\n",
        "# multiple.upload_file(\"model.onnx\", \"camera/model.onnx\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmW59CFnnBHS",
        "outputId": "c517d24c-a570-4db1-839a-3b26c7d97a03"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "device cpu\n"
          ]
        }
      ],
      "source": [
        "import onnx\n",
        "import torch\n",
        "from abraia.torch import load_classes, load_model, export_onnx\n",
        "\n",
        "dataset = 'hymenoptera_data'\n",
        "\n",
        "class_names = load_classes(os.path.join(dataset, 'model_ft.txt'))\n",
        "model = load_model(os.path.join(dataset, 'model_ft.pt'), class_names)\n",
        "model_path = export_onnx(os.path.join(dataset, 'model_ft.onnx'), model)\n",
        "# model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YkUzYb9dmcXO",
        "outputId": "c3be5bfc-92e3-4087-c1a0-0ce042bde614"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'dogs': 2.679403066635132,\n",
              " 'bees': -0.37143513560295105,\n",
              " 'ants': -1.542833685874939,\n",
              " 'cats': -1.6038011312484741}"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import onnxruntime as ort\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "ort_session = ort.InferenceSession(f\"/tmp/{model_path}\", providers=['CPUExecutionProvider'])\n",
        "\n",
        "\n",
        "def resize(img, size):\n",
        "    width = size if img.height > img.width else round(size * img.width / img.height)\n",
        "    height = round(size * img.height / img.width) if img.height > img.width else size\n",
        "    return img.resize((width, height))\n",
        "\n",
        "\n",
        "def crop(img, size):\n",
        "    left, top = (img.width - size) // 2, (img.height - size) // 2\n",
        "    right, bottom = left + size, top + size\n",
        "    return img.crop((left, top, right, bottom))\n",
        "\n",
        "\n",
        "def normalize(img, mean, std):\n",
        "    img = (np.array(img) / 255. - np.array(mean)) / np.array(std)\n",
        "    return img.astype(np.float32)\n",
        "\n",
        "\n",
        "def preprocess(img):\n",
        "    img = resize(img, 256)\n",
        "    img = crop(img, 224)\n",
        "    img = normalize(img, [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    return np.expand_dims(img.transpose((2, 0, 1)), axis=0)\n",
        "\n",
        "\n",
        "def predict(src):\n",
        "    img = Image.open(src)\n",
        "    input = preprocess(img)\n",
        "    outputs = ort_session.run(None, {\"input\": input})\n",
        "    a = np.argsort(-outputs[0].flatten())\n",
        "    results = {}\n",
        "    for i in a[0:5]:\n",
        "        results[class_names[i]]=float(outputs[0][0][i])\n",
        "    return results\n",
        "\n",
        "\n",
        "filename = 'dog.jpg'\n",
        "multiple.download_file(os.path.join(dataset, filename), filename)\n",
        "predict(filename)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
